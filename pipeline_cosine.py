import nltk
from sklearn.feature_extraction.text import CountVectorizer
import string

# Descargar stopwords si no lo tenÃ©s
nltk.download('stopwords')
from nltk.corpus import stopwords

# ðŸ“Œ Textos de ejemplo
documents = [
    """LOD Gabriel GarcÃ­a MÃ¡rquez: A Journey Through Gaboâ€™s World is a project that brings together both objects produced by Gabriel GarcÃ­a MÃ¡rquez and materials related to his figure, with the goal of building a Linked Open Data model to represent and visualize knowledge surrounding his legacy.""",
    """Another document about literature, discussing the legacy of Gabriel GarcÃ­a MÃ¡rquez and the impact of his work in Latin American culture.""",
]

query = """Gabriel GarcÃ­a MÃ¡rquez legacy"""

# ðŸ“Œ Preprocesado
stop_words = set(stopwords.words('english'))

def preprocess(text):
    # MinÃºsculas, quitar puntuaciÃ³n y stopwords
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Preprocesar documentos y consulta
processed_docs = [preprocess(doc) for doc in documents]
processed_query = preprocess(query)

# ðŸ“Œ Definir vocabulario manualmente (opcional)
vocabulario = ['gabriel', 'garcÃ­a', 'mÃ¡rquez', 'project', 'data', 'documents', 'legacy']

# ðŸ“Œ Vectorizador con vocabulario fijo
vectorizer = CountVectorizer(vocabulary=vocabulario)

# ðŸ“Œ Vectorizar documentos y consulta
doc_vectors = vectorizer.fit_transform(processed_docs).toarray()
query_vector = vectorizer.transform([processed_query]).toarray()

# ðŸ“Œ Guardar doc_vector.txt
with open('doc_vector.txt', 'w') as f:
    for idx, vec in enumerate(doc_vectors):
        f.write(f"D{idx+1}  {' '.join(map(str, vec))}\n")

# ðŸ“Œ Guardar query_vector.txt
with open('query_vector.txt', 'w') as f:
    f.write(f"Q  {' '.join(map(str, query_vector[0]))}\n")

# ðŸ“Œ Mostrar resultados en consola
print("Document Vectors:\n", doc_vectors)
print("\nQuery Vector:\n", query_vector)
